{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f13ab89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572e0539",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [[12,18,31,45,6,1],\n",
    "      [1,2,3,4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48fd2d76",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 6 at dim 1 (got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 6 at dim 1 (got 4)"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(ids) #it will throw error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "908bbf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = [[12,18,31,45,6,1],\n",
    "      [1,2,3,4,0,0]]   #padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97c94289",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(ids) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc37439",
   "metadata": {},
   "source": [
    "#### Problem with self padding(not using attention mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5a921dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc101f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = torch.tensor([[12,18,31,45,6,1,2500,1000,988,1000],\n",
    "      [1,2,3,4,1000,2999,0,0,0,0]])\n",
    "ids_1 = torch.tensor([[12,18,31,45,6,1,2500,1000,988,1000]])\n",
    "ids_2 = torch.tensor([[1,2,3,4,1000,2999]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65ce9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2aa80c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1564, -1.0404]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(ids_1).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "26995752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.7168, -1.5334]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(ids_2).logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd9519fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1564, -1.0404],\n",
      "        [ 1.0324, -0.9986]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(model(all_ids).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f73dd",
   "metadata": {},
   "source": [
    "#### You can see the difference because of just padding by zero thats why we need attention masks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e941a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.tensor([[1,1,1,1,1,1,1,1,1,1],\n",
    "                 [1,1,1,1,1,1,0,0,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cbd0fbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(all_ids, attention_mask = attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8404cdde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[ 1.1564, -1.0404],\n",
       "        [ 1.7168, -1.5334]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output # now getting right output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c4f12",
   "metadata": {},
   "source": [
    "#### Attention mask by default applied if we define padding= True in tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a14fc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "90779d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"I am danish and i love coding.\",\n",
    "            \"I love mathematics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae9ccdff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 2572, 5695, 1998, 1045, 2293, 16861, 1012, 102], [101, 1045, 2293, 5597, 102, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(sentences,padding=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ed0fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
